{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2656d92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.8085\n",
      "ROC AUC : 0.7749\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89      1593\n",
      "           1       0.59      0.19      0.28       407\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.71      0.58      0.59      2000\n",
      "weighted avg       0.78      0.81      0.77      2000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[1541   52]\n",
      " [ 331   76]]\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.8640\n",
      "ROC AUC : 0.8522\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92      1593\n",
      "           1       0.78      0.46      0.58       407\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.83      0.71      0.75      2000\n",
      "weighted avg       0.86      0.86      0.85      2000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[1541   52]\n",
      " [ 220  187]]\n",
      "\n",
      "Model: Gradient Boosting\n",
      "Accuracy: 0.8700\n",
      "ROC AUC : 0.8708\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92      1593\n",
      "           1       0.79      0.49      0.60       407\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.84      0.73      0.76      2000\n",
      "weighted avg       0.86      0.87      0.86      2000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[1541   52]\n",
      " [ 208  199]]\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE SUMMARY\n",
      "==================================================\n",
      "                     Accuracy  ROC_AUC  Precision (Churn)  Recall (Churn)\n",
      "Model                                                                    \n",
      "Logistic Regression    0.8085   0.7749             0.5938          0.1867\n",
      "Random Forest          0.8640   0.8522             0.7824          0.4595\n",
      "Gradient Boosting      0.8700   0.8708             0.7928          0.4889\n",
      "\n",
      "Notes:\n",
      "1) Gradient Boosting often gives strong ROC AUC.\n",
      "2) High accuracy can hide low recall for churners on imbalanced data.\n",
      "3) Precision shows how many predicted churners actually churned.\n",
      "4) ROC AUC reflects overall ranking quality.\n",
      "\n",
      "==================================================\n",
      "Example Prediction (Gradient Boosting)\n",
      "Predicted Churn (1=yes, 0=no): 0\n",
      "Churn Probability: 19.64%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    df = pd.read_csv(\"Churn_Modelling.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Churn_Modelling.csv not found. Using a small dummy dataset.\")\n",
    "    df = pd.DataFrame({\n",
    "        \"RowNumber\": range(10), \"CustomerId\": range(10), \"Surname\": [\"A\"] * 10,\n",
    "        \"CreditScore\": np.random.randint(400, 850, 10),\n",
    "        \"Geography\": [\"France\", \"Spain\"] * 5,\n",
    "        \"Gender\": [\"Female\", \"Male\"] * 5,\n",
    "        \"Age\": np.random.randint(20, 60, 10),\n",
    "        \"Tenure\": np.random.randint(0, 10, 10),\n",
    "        \"Balance\": np.random.rand(10) * 150000,\n",
    "        \"NumOfProducts\": np.random.randint(1, 4, 10),\n",
    "        \"HasCrCard\": np.random.randint(0, 2, 10),\n",
    "        \"IsActiveMember\": np.random.randint(0, 2, 10),\n",
    "        \"EstimatedSalary\": np.random.rand(10) * 200000,\n",
    "        \"Exited\": np.random.randint(0, 2, 10),\n",
    "    })\n",
    "\n",
    "# Basic prep\n",
    "df = df.drop([\"RowNumber\", \"CustomerId\", \"Surname\"], axis=1, errors=\"ignore\")\n",
    "X = df.drop(\"Exited\", axis=1)\n",
    "y = df[\"Exited\"]\n",
    "\n",
    "categorical_features = [\"Geography\", \"Gender\"]\n",
    "numerical_features = [c for c in X.columns if c not in categorical_features]\n",
    "\n",
    "# Transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numerical_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"), categorical_features),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "def evaluate_model(name, pipeline):\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"ROC AUC : {auc:.4f}\")\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"ROC_AUC\": auc,\n",
    "        \"Precision (Churn)\": report.get(\"1\", {}).get(\"precision\", np.nan),\n",
    "        \"Recall (Churn)\": report.get(\"1\", {}).get(\"recall\", np.nan),\n",
    "    }\n",
    "\n",
    "# Pipelines\n",
    "log_reg_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"clf\", LogisticRegression(random_state=42, solver=\"liblinear\")),\n",
    "])\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"clf\", RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "])\n",
    "\n",
    "gb_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"clf\", GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)),\n",
    "])\n",
    "\n",
    "# Run\n",
    "results = []\n",
    "results.append(evaluate_model(\"Logistic Regression\", log_reg_pipeline))\n",
    "results.append(evaluate_model(\"Random Forest\", rf_pipeline))\n",
    "results.append(evaluate_model(\"Gradient Boosting\", gb_pipeline))\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "summary_df = pd.DataFrame(results).set_index(\"Model\")\n",
    "print(summary_df.to_string(float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "print(\"\\nNotes:\")\n",
    "print(\"1) Gradient Boosting often gives strong ROC AUC.\")\n",
    "print(\"2) High accuracy can hide low recall for churners on imbalanced data.\")\n",
    "print(\"3) Precision shows how many predicted churners actually churned.\")\n",
    "print(\"4) ROC AUC reflects overall ranking quality.\")\n",
    "\n",
    "# Example prediction with the full-data Gradient Boosting pipeline\n",
    "gb_model = gb_pipeline.fit(X, y)\n",
    "new_customer = pd.DataFrame({\n",
    "    \"CreditScore\": [650], \"Geography\": [\"France\"], \"Gender\": [\"Female\"],\n",
    "    \"Age\": [40], \"Tenure\": [3], \"Balance\": [50000],\n",
    "    \"NumOfProducts\": [1], \"HasCrCard\": [1], \"IsActiveMember\": [1],\n",
    "    \"EstimatedSalary\": [100000],\n",
    "})\n",
    "pred = gb_model.predict(new_customer)[0]\n",
    "proba = gb_model.predict_proba(new_customer)[0, 1]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Example Prediction (Gradient Boosting)\")\n",
    "print(f\"Predicted Churn (1=yes, 0=no): {pred}\")\n",
    "print(f\"Churn Probability: {proba:.2%}\")\n",
    "print(\"=\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
